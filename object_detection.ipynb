{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kaefsky\\test\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "import torchvision.datasets\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.ops import nms\n",
    "from torchmetrics.detection import MeanAveragePrecision as MAP\n",
    "from PIL import ImageDraw\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights,\\\n",
    "    fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights\n",
    "from torchvision.models.detection.retinanet import retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights\n",
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights\n",
    "\n",
    "from fedot.core.pipelines.pipeline_builder import PipelineBuilder\n",
    "\n",
    "from fedcore.tools.ruler import PerformanceEvaluatorOD\n",
    "from fedcore.architecture.dataset.object_detection_datasets import YOLODataset, COCODataset\n",
    "from fedcore.architecture.comptutaional.devices import default_device\n",
    "from fedcore.architecture.utils.loader import collate\n",
    "from fedcore.data.data import CompressionInputData\n",
    "from fedcore.inference.onnx import ONNXInferenceModel\n",
    "from fedcore.neural_compressor.config import Torch2ONNXConfig\n",
    "from fedcore.repository.constanst_repository import FEDOT_TASK\n",
    "from fedcore.repository.initializer_industrial_models import FedcoreModels\n",
    "from fedcore.repository.constanst_repository import CROSS_ENTROPY, MSE\n",
    "from fedcore.architecture.visualisation.visualization import plot_train_test_loss_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = default_device()\n",
    "IMG_SIZE = 512\n",
    "NMS_THRESH = 0.6\n",
    "\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "INIT_LR = 4e-5\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "DATASET_NAME = 'african-wildlife'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kaefsky\\test\\FedCore\\datasets\\african-wildlife\n",
      "c:\\Users\\Kaefsky\\test\\FedCore\\datasets\\african-wildlife\n"
     ]
    }
   ],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    # v2.Normalize(mean=MEAN, std=STD),\n",
    "    # v2.Resize((IMG_SIZE, IMG_SIZE))\n",
    "])\n",
    "\n",
    "train_dataset = YOLODataset(dataset_name=DATASET_NAME, transform=transform, train=True, log = True)\n",
    "# train_dataset = COCODataset(images_path=COCO_PATH + \"train2017/\",\n",
    "#                             json_path=COCO_PATH + \"annotations/instances_train2017.json\",\n",
    "#                             transform=transform)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate\n",
    ")\n",
    "\n",
    "val_dataset = YOLODataset(dataset_name=DATASET_NAME, transform=transform, train=False)\n",
    "# val_dataset = COCODataset(images_path=COCO_PATH + \"val2017/\",\n",
    "#                             json_path=COCO_PATH + \"annotations/instances_val2017.json\",\n",
    "#                             transform=transform)\n",
    "val_dataset, test_dataset = torch.utils.data.random_split(val_dataset, [0.1, 0.9])\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=1, \n",
    "    shuffle=False,\n",
    "    collate_fn=collate\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=1, \n",
    "    shuffle=False,\n",
    "    collate_fn=collate\n",
    ")\n",
    "\n",
    "# More accurate, very slow to train\n",
    "# model = fasterrcnn_resnet50_fpn_v2()\n",
    "\n",
    "# Less accurate, but faster to train\n",
    "model = fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
    "\n",
    "# test\n",
    "# model = ssdlite320_mobilenet_v3_large(weights=SSDLite320_MobileNet_V3_Large_Weights.DEFAULT)\n",
    "# model = retinanet_resnet50_fpn_v2()\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes).to(device)\n",
    "model.to(device)\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=INIT_LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, mode='max', patience=3, verbose=True)\n",
    "tr_evaluator = PerformanceEvaluatorOD(model, test_loader, batch_size=1)\n",
    "val_evaluator = PerformanceEvaluatorOD(model, test_loader, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring target metric: 202batch [00:10, 18.78batch/s]\n",
      "Measuring target metric: 202batch [00:07, 25.54batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] [TRAIN] Loss: 2.362 | mAP: 0.012\n",
      "[1] [TEST] Loss: 2.368 | mAP: 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring target metric: 202batch [00:08, 24.35batch/s]\n",
      "Measuring target metric: 202batch [00:07, 25.27batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] [TRAIN] Loss: 2.099 | mAP: 0.027\n",
      "[2] [TEST] Loss: 2.248 | mAP: 0.027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring target metric: 202batch [00:08, 25.12batch/s]\n",
      "Measuring target metric: 202batch [00:07, 27.51batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] [TRAIN] Loss: 2.017 | mAP: 0.057\n",
      "[3] [TEST] Loss: 2.219 | mAP: 0.057\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring latency: 100%|██████████| 50/50 [00:01<00:00, 28.72rep/s]\n",
      "Measuring throughput: 100%|██████████| 5/5 [00:00<00:00, 28.57batch/s]\n",
      "Measuring target metric: 202batch [00:07, 26.19batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 0.17066 ms/sample with batch_size 1\n",
      "Throughput: 84746.0 samples/s with batch_size 1\n",
      "Model size: 72.458 MB\n",
      "Before quantization\n",
      "{'latency': 0.17066, 'throughput': 84746.0, 'model_size': 72.458, 'target_metrics': {'map': tensor(0.0569), 'map_50': tensor(0.1756), 'map_75': tensor(0.0116), 'map_small': tensor(0.0002), 'map_medium': tensor(0.0188), 'map_large': tensor(0.0803), 'mar_1': tensor(0.1879), 'mar_10': tensor(0.4449), 'mar_100': tensor(0.4746), 'mar_small': tensor(0.0333), 'mar_medium': tensor(0.3502), 'mar_large': tensor(0.4955), 'classes': tensor([1, 2, 3, 4], dtype=torch.int32)}}\n"
     ]
    }
   ],
   "source": [
    "tr_loss = list()\n",
    "val_loss = list() \n",
    "tr_map = list()\n",
    "val_map = list()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    \n",
    "    loss_arr = np.zeros(len(train_loader))\n",
    "    for i, (images, targets) in enumerate(train_loader):\n",
    "        # forward\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        loss_arr[i] = loss\n",
    "        # backward + optimize\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()           \n",
    "    tr_loss.append(loss_arr.mean())\n",
    "    \n",
    "    # Calculate train mAP\n",
    "    model.eval()\n",
    "    target_metric = tr_evaluator.measure_target_metric()\n",
    "    tr_map.append(float(target_metric[\"map\"]))\n",
    "            \n",
    "    # Evaluate the model\n",
    "    model.train()\n",
    "    loss_arr = np.zeros(len(test_loader)) \n",
    "    for i, (images, targets) in enumerate(test_loader):\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        loss_arr[i] = loss\n",
    "    val_loss.append(loss_arr.mean())\n",
    "    \n",
    "    # Calculate test mAP\n",
    "    model.eval()\n",
    "    target_metric = val_evaluator.measure_target_metric()\n",
    "    val_map.append(float(target_metric[\"map\"]))\n",
    "    \n",
    "    # Optimize learning rate\n",
    "    scheduler.step(float(target_metric[\"map\"]))\n",
    "    \n",
    "    # Print metrics\n",
    "    print('[%d] [TRAIN] Loss: %.3f | mAP: %.3f' %\n",
    "                    (epoch + 1, tr_loss[-1], tr_map[-1]))\n",
    "    print('[%d] [VAL]   Loss: %.3f | mAP: %.3f' %\n",
    "                    (epoch + 1, val_loss[-1], val_map[-1]))\n",
    "    \n",
    "    # Most crucial step\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    if len(val_map) > (EPOCHS // 5) and val_map[-1] <= val_map[-3]:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# Final evaluating\n",
    "performance = val_evaluator.eval()\n",
    "print('Before quantization')\n",
    "print(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_test_loss_metric(tr_loss, val_loss, tr_map, val_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = str(datetime.datetime.now())[2:-16]\n",
    "torch.save(model, f'{model._get_name()}_{DATASET_NAME}_{now}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('FasterRCNN_african-wildlife.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from fedcore.architecture.visualisation.visualization import show_image\n",
    "\n",
    "model.eval()\n",
    "model.cpu()\n",
    "val_data = val_dataset[random.randint(0, len(val_dataset) - 1)]\n",
    "img = val_data[0]\n",
    "targets = val_data[1]\n",
    "input = torch.unsqueeze(img, dim=0)\n",
    "preds = model(input)\n",
    "b_count = val_data[1]['boxes'].size(0)\n",
    "\n",
    "transform = v2.ToPILImage()\n",
    "img = transform(img)\n",
    "\n",
    "show_image(img, targets, preds, train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "repo = FedcoreModels().setup_repository()\n",
    "compression_pipeline = PipelineBuilder().add_node('post_training_quant').build()\n",
    "\n",
    "input_data = CompressionInputData(features=np.zeros((2, 2)),\n",
    "                                    idx=None,\n",
    "                                    calib_dataloader=val_loader,\n",
    "                                    task=FEDOT_TASK['regression'],\n",
    "                                    data_type=None,\n",
    "                                    target=model\n",
    ")\n",
    "\n",
    "input_data.supplementary_data.is_auto_preprocessed = True\n",
    "compression_pipeline.fit(input_data)\n",
    "quant_model = compression_pipeline.predict(input_data).predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int8_onnx_config = Torch2ONNXConfig(\n",
    "    dtype=\"int8\",\n",
    "    opset_version=18,\n",
    "    quant_format=\"QDQ\",  # or \"QLinear\"\n",
    "    example_inputs=torch.unsqueeze(train_dataset[0][0], dim=0),\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "            'input' : {0 : 'batch_size'},\n",
    "            'output' : {0 : 'batch_size'}\n",
    "        }\n",
    ")\n",
    "\n",
    "quant_model.export(\"int8-model.onnx\", int8_onnx_config)\n",
    "onnx_model = ONNXInferenceModel(\"int8-model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = PerformanceEvaluatorOD(model, test_loader, batch_size=1)\n",
    "performance = evaluator.eval()\n",
    "print('after quantization')\n",
    "print(performance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
