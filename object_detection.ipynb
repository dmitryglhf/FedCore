{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from fedcore.models.backbone.mobilenet import MobileNet\n",
    "from fedcore.models.backbone.resnet import ResNetModel\n",
    "from fedcore.models.backbone.resnet import ObjectDetector\n",
    "from fedcore.architecture.dataset.object_detection_datasets import YOLODataset, KittiCustomDataset, LabelEncoder\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn\n",
    "import torchvision.datasets\n",
    "from fedot.core.pipelines.pipeline_builder import PipelineBuilder\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from fedcore.architecture.comptutaional.devices import default_device\n",
    "from fedcore.architecture.utils.paths import data_path\n",
    "from fedcore.data.data import CompressionInputData\n",
    "from fedcore.inference.onnx import ONNXInferenceModel\n",
    "from fedcore.neural_compressor.config import Torch2ONNXConfig\n",
    "from fedcore.repository.constanst_repository import FEDOT_TASK\n",
    "from fedcore.repository.initializer_industrial_models import FedcoreModels\n",
    "from fedcore.repository.constanst_repository import CROSS_ENTROPY, MSE\n",
    "\n",
    "from fedcore.tools.ruler import PerformanceEvaluatorOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "# import torch\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# define the base path to the input dataset and then use it to derive\n",
    "# the path to the input images and annotation CSV files\n",
    "BASE_PATH = \"datasets/kitti\"\n",
    "TRAIN_IMAGES_PATH = os.path.altsep.join([BASE_PATH, \"train/img/\"])\n",
    "TRAIN_LABELS_PATH = os.path.altsep.join([BASE_PATH, \"train/ann/\"])\n",
    "TRAIN_LENGTH = 7480\n",
    "# define the path to the base output directory\n",
    "BASE_OUTPUT = \"output\"\n",
    "# define the path to the output model, label encoder, plots output\n",
    "# directory, and testing image paths\n",
    "MODEL_PATH = os.path.sep.join([BASE_OUTPUT, \"detector.pth\"])\n",
    "LE_PATH = os.path.sep.join([BASE_OUTPUT, \"le.pickle\"])\n",
    "PLOTS_PATH = os.path.sep.join([BASE_OUTPUT, \"plots\"])\n",
    "TEST_PATHS = os.path.sep.join([BASE_OUTPUT, \"test_paths.txt\"])\n",
    "\n",
    "# these are standard deviation and Mean from ImageNet dataset\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "\n",
    "# learning rate\n",
    "INIT_LR = 0.002\n",
    "\n",
    "# epochs\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# bbox and label class loss\n",
    "BBOX = 1.0\n",
    "LABELS = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = torch.stack([sample[0] for sample in batch], dim=0)\n",
    "    labels = [F.pad(sample[1]['labels'], (0, 8 - sample[1]['boxes'].shape[0])) for sample in batch]\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    bboxes = [F.pad(sample[1]['boxes'], (0, 0, 0, 8 - sample[1]['boxes'].shape[0])) for sample in batch]\n",
    "    bboxes = torch.stack(bboxes, dim=0)\n",
    "    return images, labels, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 4860.821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:01<00:22,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,     1] loss: 4875.154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:02<00:21,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3,     1] loss: 4861.874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:04<00:22,  1.35s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m classLoss \u001b[38;5;241m=\u001b[39m classLossFunc(abs_predictions, source_pad)\n\u001b[0;32m     59\u001b[0m totalLoss \u001b[38;5;241m=\u001b[39m (BBOX \u001b[38;5;241m*\u001b[39m bboxLoss) \u001b[38;5;241m+\u001b[39m (LABELS \u001b[38;5;241m*\u001b[39m classLoss)\n\u001b[1;32m---> 61\u001b[0m \u001b[43mtotalLoss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# print statistics\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kaefsky\\Python\\Fedcore\\.venv\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kaefsky\\Python\\Fedcore\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    transform = v2.Compose([\n",
    "        v2.ToPILImage(),\n",
    "        v2.ToTensor(),\n",
    "        v2.Normalize(mean=MEAN, std=STD),\n",
    "        v2.Resize((640, 480))\n",
    "    ])\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "   \n",
    "    train_dataset = YOLODataset(path=\"datasets\\coco8\\coco8.yaml\", transform=transform, train=True)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    val_dataset = YOLODataset(path=\"datasets\\coco8\\coco8.yaml\", transform=transform, train=True)\n",
    "    val_dataset, test_dataset = torch.utils.data.random_split(val_dataset, [0.1, 0.9])\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=1, \n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    model = resnet18(pretrained=True).to(DEVICE)\n",
    "    model.fc = nn.Linear(512, 10).to(DEVICE)\n",
    "    \n",
    "    # create our custom object detector model and move it to the current device\n",
    "    objectDetector = ObjectDetector(model, 79)\n",
    "    objectDetector = objectDetector.to(DEVICE)\n",
    "    \n",
    "    bboxLossFunc = MSE()\n",
    "    classLossFunc = CROSS_ENTROPY()\n",
    "    opt = Adam(objectDetector.parameters(), lr=INIT_LR)\n",
    "    model.train()\n",
    "    # Train the model\n",
    "    for epoch in tqdm(range(NUM_EPOCHS)):  # loop over the dataset multiple times\n",
    "        objectDetector.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels, bboxes, _) in enumerate(train_dataloader, 0):\n",
    "            # send input to device\n",
    "            (images, labels, bboxes) = (images.to(DEVICE),\n",
    "\t\t\tlabels.to(default_device()), bboxes.to(DEVICE))\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            outputs = objectDetector(images)\n",
    "            bboxLoss = bboxLossFunc(outputs[0], bboxes)\n",
    "            \n",
    "            source_pad = torch.nn.functional.pad(labels, pad=(0, 9 - labels.shape[1], 0, 0))\n",
    "            source_pad = source_pad.type(torch.float32)\n",
    "            abs_predictions = torch.abs(outputs[1])\n",
    "            abs_predictions = abs_predictions.type(torch.float32)\n",
    "            \n",
    "            classLoss = classLossFunc(abs_predictions, source_pad)\n",
    "            totalLoss = (BBOX * bboxLoss) + (LABELS * classLoss)\n",
    "            \n",
    "            totalLoss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += totalLoss.item()\n",
    "            if i % 20 == 0:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 20))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    evaluator = PerformanceEvaluatorOD(objectDetector, test_dataset, batch_size=1)\n",
    "    performance = evaluator.eval()\n",
    "    print('Before quantization')\n",
    "    print(performance)\n",
    "    \n",
    "    model = model.cpu()\n",
    "    repo = FedcoreModels().setup_repository()\n",
    "    compression_pipeline = PipelineBuilder().add_node('post_training_quant').build()\n",
    "\n",
    "    input_data = CompressionInputData(features=np.zeros((2, 2)),\n",
    "                                      idx=None,\n",
    "                                      calib_dataloader=val_dataloader,\n",
    "                                      task=FEDOT_TASK['classification'],\n",
    "                                      data_type=None,\n",
    "                                      target=model\n",
    "                                      )\n",
    "    \n",
    "    input_data.supplementary_data.is_auto_preprocessed = True\n",
    "    compression_pipeline.fit(input_data)\n",
    "    quant_model = compression_pipeline.predict(input_data).predict\n",
    "\n",
    "    int8_onnx_config = Torch2ONNXConfig(\n",
    "        dtype=\"int8\",\n",
    "        opset_version=16,\n",
    "        quant_format=\"QDQ\",  # or \"QLinear\"\n",
    "        example_inputs=torch.unsqueeze(train_dataset[0][0], dim=0),\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes={'input': [0], 'output': [0]}\n",
    "    )\n",
    "\n",
    "    quant_model.export(\"int8-model.onnx\", int8_onnx_config)\n",
    "    onnx_model = ONNXInferenceModel(\"int8-model.onnx\")\n",
    "    \n",
    "    evaluator = PerformanceEvaluatorOD(onnx_model,  test_dataset, batch_size=64)\n",
    "    performance = evaluator.eval()\n",
    "    print('after quantization')\n",
    "    print(performance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
