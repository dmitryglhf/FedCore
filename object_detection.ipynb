{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kaefsky\\Python\\Fedcore\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "import torchvision.datasets\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.ops import nms\n",
    "from torchmetrics.detection import MeanAveragePrecision as MAP\n",
    "from PIL import ImageDraw\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights,\\\n",
    "    fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights\n",
    "from torchvision.models.detection.retinanet import retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights\n",
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights\n",
    "\n",
    "from fedot.core.pipelines.pipeline_builder import PipelineBuilder\n",
    "\n",
    "from fedcore.tools.ruler import PerformanceEvaluatorOD\n",
    "from fedcore.architecture.dataset.object_detection_datasets import YOLODataset, COCODataset, UnlabeledDataset\n",
    "from fedcore.architecture.comptutaional.devices import default_device\n",
    "from fedcore.architecture.utils.loader import collate\n",
    "from fedcore.data.data import CompressionInputData\n",
    "from fedcore.inference.onnx import ONNXInferenceModel\n",
    "from fedcore.neural_compressor.config import Torch2ONNXConfig\n",
    "from fedcore.repository.constanst_repository import FEDOT_TASK\n",
    "from fedcore.repository.initializer_industrial_models import FedcoreModels\n",
    "from fedcore.repository.constanst_repository import CROSS_ENTROPY, MSE\n",
    "from fedcore.architecture.visualisation.visualization import plot_train_test_loss_metric, apply_nms, get_image, filter_boxes\n",
    "from fedcore.architecture.utils.loader import get_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = default_device()\n",
    "IMG_SIZE = 512\n",
    "NMS_THRESH = 0.5\n",
    "THRESH = 0.5\n",
    "\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "INIT_LR = 4e-5\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "DATASET_NAME = 'african-wildlife'\n",
    "OUTPUT_PATH = f'datasets/{DATASET_NAME}/output/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Path:  c:\\Users\\Kaefsky\\Python\\Fedcore\\FedCore\\datasets\\african-wildlife\n"
     ]
    }
   ],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    # v2.Normalize(mean=MEAN, std=STD),\n",
    "    # v2.Resize((IMG_SIZE, IMG_SIZE))\n",
    "])\n",
    "\n",
    "train_dataset = YOLODataset(dataset_name=DATASET_NAME, transform=transform, train=True, log = True)\n",
    "# train_dataset = COCODataset(images_path=COCO_PATH + \"train2017/\",\n",
    "#                             json_path=COCO_PATH + \"annotations/instances_train2017.json\",\n",
    "#                             transform=transform)\n",
    "\n",
    "test_dataset = YOLODataset(dataset_name=DATASET_NAME, transform=transform, train=False)\n",
    "# val_dataset = COCODataset(images_path=COCO_PATH + \"val2017/\",\n",
    "#                             json_path=COCO_PATH + \"annotations/instances_val2017.json\",\n",
    "#                             transform=transform)\n",
    "val_dataset = UnlabeledDataset(images_path=f'datasets/{DATASET_NAME}/valid/images/')\n",
    "\n",
    "\n",
    "train_loader = get_loader(train_dataset, batch_size=BATCH_SIZE, train=True)\n",
    "test_loader = get_loader(test_dataset)\n",
    "val_loader = get_loader(val_dataset)\n",
    "\n",
    "# More accurate, very slow to train\n",
    "# model = fasterrcnn_resnet50_fpn_v2()\n",
    "\n",
    "# Less accurate, but faster to train\n",
    "model = fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
    "\n",
    "# test\n",
    "# model = ssdlite320_mobilenet_v3_large(weights=SSDLite320_MobileNet_V3_Large_Weights.DEFAULT)\n",
    "# model = retinanet_resnet50_fpn_v2()\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes).to(device)\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=INIT_LR, momentum=0.9, weight_decay=INIT_LR/2)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, mode='max', patience=3, verbose=True)\n",
    "tr_evaluator = PerformanceEvaluatorOD(model, train_loader, batch_size=BATCH_SIZE)\n",
    "test_evaluator = PerformanceEvaluatorOD(model, test_loader, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring target metric: 202batch [00:10, 18.78batch/s]\n",
      "Measuring target metric: 202batch [00:07, 25.54batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] [TRAIN] Loss: 2.362 | mAP: 0.012\n",
      "[1] [TEST] Loss: 2.368 | mAP: 0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring target metric: 202batch [00:08, 24.35batch/s]\n",
      "Measuring target metric: 202batch [00:07, 25.27batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] [TRAIN] Loss: 2.099 | mAP: 0.027\n",
      "[2] [TEST] Loss: 2.248 | mAP: 0.027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring target metric: 202batch [00:08, 25.12batch/s]\n",
      "Measuring target metric: 202batch [00:07, 27.51batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] [TRAIN] Loss: 2.017 | mAP: 0.057\n",
      "[3] [TEST] Loss: 2.219 | mAP: 0.057\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring latency: 100%|██████████| 50/50 [00:01<00:00, 28.72rep/s]\n",
      "Measuring throughput: 100%|██████████| 5/5 [00:00<00:00, 28.57batch/s]\n",
      "Measuring target metric: 202batch [00:07, 26.19batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 0.17066 ms/sample with batch_size 1\n",
      "Throughput: 84746.0 samples/s with batch_size 1\n",
      "Model size: 72.458 MB\n",
      "Before quantization\n",
      "{'latency': 0.17066, 'throughput': 84746.0, 'model_size': 72.458, 'target_metrics': {'map': tensor(0.0569), 'map_50': tensor(0.1756), 'map_75': tensor(0.0116), 'map_small': tensor(0.0002), 'map_medium': tensor(0.0188), 'map_large': tensor(0.0803), 'mar_1': tensor(0.1879), 'mar_10': tensor(0.4449), 'mar_100': tensor(0.4746), 'mar_small': tensor(0.0333), 'mar_medium': tensor(0.3502), 'mar_large': tensor(0.4955), 'classes': tensor([1, 2, 3, 4], dtype=torch.int32)}}\n"
     ]
    }
   ],
   "source": [
    "tr_loss = np.zeros(EPOCHS)\n",
    "test_loss = np.zeros(EPOCHS)\n",
    "tr_map = np.zeros(EPOCHS)\n",
    "test_map = np.zeros(EPOCHS)\n",
    "train_time = np.zeros(EPOCHS)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    tStart = time.time()\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    \n",
    "    loss_arr = np.zeros(len(train_loader))\n",
    "    for i, (images, targets) in enumerate(train_loader):\n",
    "        # forward\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        loss_arr[i] = loss\n",
    "        # backward + optimize\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()           \n",
    "    tr_loss[epoch] = loss_arr.mean()\n",
    "    \n",
    "    # Calculate train mAP\n",
    "    model.eval()\n",
    "    target_metric = tr_evaluator.measure_target_metric()\n",
    "    tr_map[epoch] = float(target_metric[\"map\"])\n",
    "            \n",
    "    # Evaluate the model\n",
    "    model.train()\n",
    "    loss_arr = np.zeros(len(test_loader)) \n",
    "    for i, (images, targets) in enumerate(test_loader):\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        loss_arr[i] = loss\n",
    "    test_loss[epoch] = loss_arr.mean()\n",
    "    \n",
    "    # Calculate test mAP\n",
    "    model.eval()\n",
    "    target_metric = test_evaluator.measure_target_metric()\n",
    "    test_map[epoch] = float(target_metric[\"map\"])\n",
    "    \n",
    "    # Optimize learning rate\n",
    "    scheduler.step(test_map[epoch])\n",
    "    \n",
    "    tEnd = time.time()\n",
    "    train_time[epoch] = float(tEnd - tStart)\n",
    "    \n",
    "    # Print metrics\n",
    "    p = int(math.log(epoch + 1, 10))\n",
    "    print('-' * (40 + p))\n",
    "    print('| %d | TRAIN | Loss: %.3f | mAP: %.3f |' %\n",
    "            (epoch + 1, tr_loss[epoch], tr_map[epoch]))\n",
    "    print('| %d | TEST  | Loss: %.3f | mAP: %.3f |' %\n",
    "            (epoch + 1, test_loss[epoch], test_map[epoch]))\n",
    "    print('-' * (13 + p), \n",
    "            'Time: %.2f' % train_time[epoch], \n",
    "            '-' * 14)\n",
    "    \n",
    "    # Saving best model\n",
    "    if test_map[epoch].max():\n",
    "        best_model = model\n",
    "    \n",
    "    # Most crucial step\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    if epoch > 5 and test_map[epoch] <= test_map[epoch - 5]:\n",
    "        tr_loss = tr_loss[:epoch + 1]\n",
    "        test_loss = test_loss[:epoch + 1]\n",
    "        tr_map = tr_map[:epoch + 1]\n",
    "        test_map = test_map[:epoch + 1]\n",
    "        train_time = train_time[:epoch + 1]\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# Final evaluating\n",
    "model = best_model\n",
    "performance = test_evaluator.eval()\n",
    "print('Before quantization')\n",
    "print(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_test_loss_metric(tr_loss, test_loss, tr_map, test_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = str(datetime.datetime.now())[2:-16]\n",
    "torch.save(model, f'{model._get_name()}_{DATASET_NAME}_{now}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('FasterRCNN_african-wildlife_24-07-07.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "id = random.randint(0, len(val_dataset) - 1) # random or int\n",
    "test_data = test_loader.dataset[id]\n",
    "img, target = test_data\n",
    "input = torch.unsqueeze(img, dim=0)\n",
    "pred = model(input)\n",
    "pred = apply_nms(pred[0], NMS_THRESH)\n",
    "pred = filter_boxes(pred, THRESH)\n",
    "\n",
    "# Show inference image\n",
    "transform = v2.ToPILImage()\n",
    "img = transform(img)\n",
    "inference_img = get_image(img, pred, train_dataset.classes, target)\n",
    "inference_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting all inference images\n",
    "for data in val_loader:\n",
    "    image = data[0][0].cpu()\n",
    "    name = data[1][0]['name']\n",
    "    input = torch.unsqueeze(image, dim=0)\n",
    "    pred = model(input)\n",
    "    pred = apply_nms(pred[0], NMS_THRESH)\n",
    "    pred = filter_boxes(pred, THRESH)\n",
    "    transform = v2.ToPILImage()\n",
    "    img = transform(image)\n",
    "    inference_img = get_image(img, pred, train_dataset.classes)\n",
    "    if not os.path.exists(OUTPUT_PATH):\n",
    "        os.makedirs(OUTPUT_PATH)\n",
    "    inference_img.save(OUTPUT_PATH + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "repo = FedcoreModels().setup_repository()\n",
    "compression_pipeline = PipelineBuilder().add_node('post_training_quant').build()\n",
    "\n",
    "input_data = CompressionInputData(features=np.zeros((2, 2)),\n",
    "                                    idx=None,\n",
    "                                    calib_dataloader=val_loader,\n",
    "                                    task=FEDOT_TASK['regression'],\n",
    "                                    data_type=None,\n",
    "                                    target=model\n",
    ")\n",
    "\n",
    "input_data.supplementary_data.is_auto_preprocessed = True\n",
    "compression_pipeline.fit(input_data)\n",
    "quant_model = compression_pipeline.predict(input_data).predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int8_onnx_config = Torch2ONNXConfig(\n",
    "    dtype=\"int8\",\n",
    "    opset_version=18,\n",
    "    quant_format=\"QDQ\",  # or \"QLinear\"\n",
    "    example_inputs=torch.unsqueeze(train_dataset[0][0], dim=0),\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "            'input' : {0 : 'batch_size'},\n",
    "            'output' : {0 : 'batch_size'}\n",
    "        }\n",
    ")\n",
    "\n",
    "quant_model.export(\"int8-model.onnx\", int8_onnx_config)\n",
    "onnx_model = ONNXInferenceModel(\"int8-model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = PerformanceEvaluatorOD(model, test_loader, batch_size=1)\n",
    "performance = evaluator.eval()\n",
    "print('after quantization')\n",
    "print(performance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
